# numerical_optimization_ai
Developed custom gradient-based optimization solvers (SGD, Adam) with adaptive learning rates. Implemented regularization techniques and convergence criteria monitoring for robust model training. Benchmarked against standard library implementations, achieving comparable performance with greater flexibility.
